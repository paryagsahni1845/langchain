{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f2e4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9937b7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F241A76BA0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F241A77110>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm= ChatGroq(api_key=api_key,model=\"llama-3.3-70b-versatile\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da74e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,HumanMessage,SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa44b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech=\"\"\"\n",
    "Summarization is one of the most important tasks in natural language processing because the world is drowning in information—research papers, articles, reports, meeting transcripts—and humans don’t always have the time to read everything end-to-end. Summarization helps by condensing long text into shorter, meaningful outputs while still retaining the essence of the original.\n",
    "\n",
    "There are two classical approaches:\n",
    "\n",
    "Extractive Summarization – The system identifies the most important sentences or phrases in the original text and directly extracts them. Think of it as a highlighter tool—it doesn’t create new sentences, it just pulls out the best ones. Algorithms like TextRank, LexRank, or TF-IDF based methods were popular here.\n",
    "\n",
    "Abstractive Summarization – Instead of copy-pasting sentences, the system rewrites the content in new words, just like a human would. This requires language understanding and generation, which became possible with transformer-based models like BERT, GPT, and T5.\n",
    "\n",
    "Now, LLMs like GPT, LLaMA, or Claude are excellent at abstractive summarization. They don’t simply pick lines—they generate new text token by token using a feed-forward neural network + attention + softmax decoding loop. This means they can create fluent summaries, paraphrase ideas, and even adapt tone or length.\n",
    "\n",
    "But here’s the catch: LLMs have token limits. A model can’t always read a 200-page PDF in one go. That’s where frameworks like LangChain come in with smart strategies:\n",
    "\n",
    "Map-Reduce Summarization → Break the text into chunks. Summarize each chunk individually (“map”), then summarize all those summaries together (“reduce”).\n",
    "\n",
    "Refine Summarization → Summarize the first chunk, then iteratively refine the summary as you process more chunks, like updating a running draft.\n",
    "\n",
    "Stuff Summarization → Simply stuff as much text as possible into the model’s context window and ask for a summary. Works for small docs.\n",
    "\n",
    "Additionally, LLM-powered summarizers can be goal-directed:\n",
    "\n",
    "Bullet point summaries for clarity.\n",
    "\n",
    "Executive summaries for decision makers.\n",
    "\n",
    "Detailed summaries for researchers.\n",
    "\n",
    "Conversational summaries for chatbots.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8302a2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSummarization is one of the most important tasks in natural language processing because the world is drowning in information—research papers, articles, reports, meeting transcripts—and humans don’t always have the time to read everything end-to-end. Summarization helps by condensing long text into shorter, meaningful outputs while still retaining the essence of the original.\\n\\nThere are two classical approaches:\\n\\nExtractive Summarization – The system identifies the most important sentences or phrases in the original text and directly extracts them. Think of it as a highlighter tool—it doesn’t create new sentences, it just pulls out the best ones. Algorithms like TextRank, LexRank, or TF-IDF based methods were popular here.\\n\\nAbstractive Summarization – Instead of copy-pasting sentences, the system rewrites the content in new words, just like a human would. This requires language understanding and generation, which became possible with transformer-based models like BERT, GPT, and T5.\\n\\nNow, LLMs like GPT, LLaMA, or Claude are excellent at abstractive summarization. They don’t simply pick lines—they generate new text token by token using a feed-forward neural network + attention + softmax decoding loop. This means they can create fluent summaries, paraphrase ideas, and even adapt tone or length.\\n\\nBut here’s the catch: LLMs have token limits. A model can’t always read a 200-page PDF in one go. That’s where frameworks like LangChain come in with smart strategies:\\n\\nMap-Reduce Summarization → Break the text into chunks. Summarize each chunk individually (“map”), then summarize all those summaries together (“reduce”).\\n\\nRefine Summarization → Summarize the first chunk, then iteratively refine the summary as you process more chunks, like updating a running draft.\\n\\nStuff Summarization → Simply stuff as much text as possible into the model’s context window and ask for a summary. Works for small docs.\\n\\nAdditionally, LLM-powered summarizers can be goal-directed:\\n\\nBullet point summaries for clarity.\\n\\nExecutive summaries for decision makers.\\n\\nDetailed summaries for researchers.\\n\\nConversational summaries for chatbots.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee63fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message=[\n",
    "    SystemMessage(content=\"you are expert in summarizing text\"),\n",
    "    HumanMessage(content=f\"please provide summary of speech : {speech}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382ff642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain\\lenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cef86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_23608\\1339791998.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(chat_message).content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here is a summary of the speech:\\n\\n**Importance of Summarization**: With the overwhelming amount of information, summarization is crucial to condense long texts into shorter, meaningful outputs while retaining the original essence.\\n\\n**Two Classical Approaches**:\\n\\n1. **Extractive Summarization**: Identifies and extracts important sentences or phrases.\\n2. **Abstractive Summarization**: Rewrites content in new words, like a human, using transformer-based models like BERT, GPT, and T5.\\n\\n**Large Language Models (LLMs)**: Excellent at abstractive summarization, generating fluent summaries, paraphrasing ideas, and adapting tone or length. However, they have token limits.\\n\\n**Strategies to Overcome Token Limits**:\\n\\n1. **Map-Reduce Summarization**: Breaks text into chunks, summarizes each chunk, and then summarizes all summaries.\\n2. **Refine Summarization**: Iteratively refines the summary as more chunks are processed.\\n3. **Stuff Summarization**: Fits as much text as possible into the model's context window.\\n\\n**Goal-Directed Summarizers**: Can be used for various purposes, such as bullet point summaries, executive summaries, detailed summaries, and conversational summaries.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
